\documentclass[a4paper,oneside]{report}
\usepackage[spanish]{babel}
\usepackage[latin1]{inputenc}
\usepackage{fullpage}
\usepackage{listings}
\usepackage{gmverb}
\usepackage[colorlinks=true,urlcolor=black,linkcolor=black]{hyperref}%
\usepackage{listings}

\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

\lstset{language=,keywordstyle=\ttfamily,stringstyle=\ttfamily}
\lstset{breaklines}

\title{Inteligencia Artificial\\Comportamiento del agente}

\author{Diego Marcovecchio (LU: 83815)\and Tomás Touceda (LU: 84024)}

\date{29 de Octubre de 2010}

\begin{document}
\lstset{frame=single}	
\maketitle
		
\tableofcontents

\chapter{Introducción}
\section{Descripción}

Este proyecto consiste en la implementación del comportamiento del agente Bugor en el entorno provisto por la cátedra. El comportamiento del agente se decide dinámicamente en cada turno en base a la percepción que se mostró durante la primer entrega, y los objetivos principales del agente son explorar la mayor cantidad del mapa posible, obtener la mayor cantidad de tesoros, y aumentar su fight\_skill de la manera menos peligrosa posible.

\section{Modularización}
Para facilitar la lectura del código, la implementación de Bugor está separada en diferentes módulos en la carpeta /bugorLib. A continuación describiremos brevemente el contenido de cada uno, y más adelante se detallarán las estrategias empleadas:
\begin{itemize}
	\item \textbf{astar.pl}: incluye la implementación del algoritmo de búsqueda A*.
	\item \textbf{auxiliares.pl}: contiene predicados auxiliares y de debugging.
	\item \textbf{behave.pl}: el código de este módulo es el encargado de decidir cuáles de las estrategias implementadas seguirá el agente. Cada estrategia tiene un módulo propio de implementación, y en este únicamente se decide cuál es conveniente utilizar.
	\item \textbf{percept.pl}: incluye la programación de la percepción del agente.
	\item \textbf{/strategies}	
		\begin{itemize}
			\item attack.pl: contiene el código de la estrategia de ataque del agente.
			\item flee.pl: incluye el código utilizado cuando el agente es atacado por otros.
			\item fleehostel.pl: contiene el código del agente para dirigirse inmediatamente al hotel más cercano.
			\item general.pl: incluye el código del agente dedicado a explorar las zonas desconocidas del mapa.
			\item treasures.pl: contiene la implementación de la búsqueda y recolección de tesoros por parte del agente.
		\end{itemize}
\end{itemize}

\chapter{Mejoras en la percepción}

Si bien la base de la percepción fue diseñada en la primer entrega del proyecto, se realizaron algunas correcciones y mejoras en ésta.

Por empezar, se corrigió un bug que resultaba en que si un agente tiraba al suelo un tesoro, la cantidad de tesoros mantenida por dicho agente podía ser negativa. Por otro lado, también se agregó el hecho de que los ataques por la espalda a Bugor queden también registrados en la caracterización de los agentes rivales.

Se agregó, además, la utilización del predicado dinámico \emph{sight/1} que refleja de manera conveniente el campo visual de Bugor. Este predicado se actualiza todos los turnos, y es utilizado en la mayoría de las estrategias para decidir la acción a realizar.

Por último, ahora también contamos con un predicado dinámico \emph{me/5}, que también se actualiza todos los turnos, y contiene la información del estado del agente (posición actual, dirección actual, stamina, stamina máxima y fight\_skill).

\chapter{Búsqueda}
\section{Características del algoritmo}
En todos los casos, el algoritmo utilizado para encontrar caminos es A*.

En nuestra implementación, el algoritmo tiene una única meta; esto significa que para decidir cuál es el camino más corto entre un conjunto de \emph{n} metas, se realizan \emph{n} ejecuciones del algoritmo, y se comparan los costos de cada una, seleccionando finalmente la meta que resulte más barata. Una alternativa posible es implementar el algoritmo para buscar en un conjunto de metas, lo cual significaría cortar antes el proceso de búsqueda para metas más cercanas; el problema es que esta implementación, con muchas metas lejanas, resultaba en un costo de memoria mucho mayor al deseado, y la ganancia en velocidad de cómputo del camino no es tan grande (ambas posibilidades tienen el mismo orden de ejecución).

\section{Representación de nodos}

Los nodos en el algoritmo de búsqueda están representados por su posición en el mapa, el costo parcial asociado, el nodo antecesor y una dirección. La posición es una lista \emph{[X,Y]} con las coordenadas; el nodo antecesor indica desde qué nodo se pasó al nodo actual; la dirección indica hacia qué punto cardinal estará mirando el agente, y el costo parcial asociado se calcula según el costo del nodo antecesor, más el costo de desplazamiento (1 si es \emph{plain}, 2 si es \emph{mountain}) más el costo de giro (0 si la dirección en el nodo es la misma que la dirección en el padre, 1 si es necesario realizar un giro de un nodo al otro). Nótese que es posible construir el path a un nodo concatenando recursivamente todos los antecesores.

Los únicos nodos que se generan son aquellas posiciones del mapa cuyo terreno es \emph{plain} o \emph{mountain}. De esta manera, los bosques, el agua, y los trozos de mapa no explorados no son considerados en la búsqueda. Esto convierte a nuestra implementación en un algoritmo pesimista, dado que asume que no se puede caminar por las partes desconocidas del mapa. Se encuentra definida, adicionalmente, una constante para indicar que el costo de un camino es "`infinito"', utilizada para indicar que un camino dado no puede llevar a la meta.

\section{Función heurística}

La función heurística empleada en nuestro A* es la \emph{distancia manhattan} de un nodo a la meta. Dado que la meta es única, esta función no tiene ningún tipo de complicaciones, y garantiza subestimar el costo de movimiento.

\section{Control de nodos visitados}

Para evitar caer en ciclos y obtener siempre el camino minimal, el algoritmo utiliza el predicado dinámico \emph{visitados/1}. Cada vez que se generan los vecinos de un nodo, se checkea si las posiciones de cada uno de ellos ya fueron visitadas o están en la frontera; en cualquiera de estos dos casos, si el costo del nodo visitado o en frontera es mayor al del camino actual, es borrado y reemplazado por el nuevo.

\section{Resultado del algoritmo}

De existir un camino por las partes ya exploradas del mapa hacia la meta deseada, el algoritmo siempre lo encuentra; el resultado de la aplicación de nuestro A* es una lista de nodos; en la práctica, como se verá más adelante, dicha lista se traduce a una secuencia de acciones a realizar.

En caso de que sea absolutamente necesario cruzar por una zona desconocida del mapa, el algoritmo devolverá un camino vacío y con el costo "`infinito"' definido por la constante mencionada anteriormente.

\section{Ejemplos}

PONÉ LOS EJEMPLOS, PELADOOOOOOOOOO... BIOOOOOOOOOOOMMMMMMMMMMMMMMMMMMM!

\chapter{Estrategias}

Dependiendo del momento, el estado y la percepción del mundo, Bugor puede utilizar diferentes estrategias de comportamiento para lograr sus intenciones (principalmente: no morir, conseguir oro, explorar y mejorar su fight\_skill). El manejo de las estrategias en el agente es realizado mediante una pila: según sea necesario, se apila una estrategia, se trabaja de acuerdo a ella, y posteriormente se desapila y se continúa trabajando con la estrategia anterior. Cuando la pila de estrategias se encuentra vacía, por default se apila la estrategia de exploración.

Cada una de las estrategias tiene una prioridad definida, y únicamente se puede apilar una estrategia encima de otra cuando ésta tiene mayor o igual prioridad. Esta característica es la que indicará si una estrategia espera a que otra termine para comenzar a actuar, o si puede interrumpir el desarrollo de otra estrategia.

El agente además posee una pila de acciones, que corresponden siempre a la estrategia en el tope de la pila de estrategias. Bugor utilizará esta pila para planificar sus movimientos futuros dentro del mapa.

Pasaremos ahora a describir las estrategias implementadas.

\section{General}
La estrategia "`general"' (o "`estrategia de exploración"') es utilizada por Bugor para explorar pedazos de mapa que no conocía anteriormente. Sabiendo que el agente posee una representación interna del mapa del mundo, el primer paso es obtener todos los \emph{nodos frontera} que sean \emph{plain} o \emph{mountain} en dicho mapa interno. Un nodo es considerado \emph{frontera} cuando tiene al menos un vecino que no se conoce.

A continuación, todos estos nodos son ordenados según la \emph{distancia manhattan} respecto a la posición actual de Bugor. El nodo frontera con la menor distancia manhattan es marcado como meta, y el agente se dirige hacia allí.

La decisión de ordenar los nodos heurísticamente y no elegirlos de manera aleatoria se tomó para evitar que el agente se dirija hacia una posición muy lejana por el simple hecho de explorar; resulta más provechoso explorar repetidas veces nodos cercanos en la frontera que explorar una única posición de la frontera muy lejana.

En el caso de que el nodo elegido no pueda ser accedido (es decir, cuando el costo de A* aplicado a ese nodo es la constante que representa \emph{infinito}), se prosigue a explorar el siguiente nodo con menor heurística de la lista.

Mientras el agente esté aplicando esta estrategia, siempre que vea oro dentro de su campo visual, intentará ir a buscarlo inmediatamente; para ésto se apila la estrategia \emph{Treasures} que se describe a continuación en la pila de estrategias.

Una vez que el mapa esté completamente explorado, si el agente recuerda la existencia de tesoros que no pudo recoger, va en busca de ellos. Para definir cuál de los tesoros será buscado se realiza un balance entre la cantidad de turnos que pasaron desde la última vez que éste fue visto, y el costo de ir a buscarlo.

Por último, en caso de que el mapa ya esté completamente explorado y Bugor no recuerde la existencia de tesoros pendientesde ser recogidos, simplemente se desplazará al azar por el mapa.

\section{Treasures}

Esta estrategia es la encargada de definir el comportamiento de Bugor al recoger un tesoro.

El agente está diseñado para apilar esta estrategia \textbf{únicamente} cuando tiene un tesoro dentro de su campo visual. Dado ese caso, Bugor intentará ir a buscarlo inmediatamente. En caso de que dicho tesoro sea alcanzable caminando por las porciones de mapa conocido, se apila la secuencia de acciones a realizar para llegar a dicho tesoro en la pila de acciones, se realizan inmediatamente para intentar recogerlo, y se continúa explorando (es decir, se desapila la estrategia Treasures de la pila). Si el tesoro es, por el momento, inalcanzable, se prosigue explorando como se planeó originalmente y el tesoro queda igualmente recordado en la "`memoria"' del agente. %esto está bien? o EN ESTA ESTRATEGIA va a buscar el tesoro más cercano que tiene?


\section{Flee}

Bugor no es un agente particularmente agresivo, y si bien intentará combatir (como se detallará más adelante), la estrategia principal en caso de recibir un ataque es escapar lo más rápido y lejos posible.

Para realizar esto, inmediatamente después de recibir un ataque, Bugor apilará esta estrategia

\section{}

\end{document}